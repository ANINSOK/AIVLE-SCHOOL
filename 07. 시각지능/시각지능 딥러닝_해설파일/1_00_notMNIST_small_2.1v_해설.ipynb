{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvkPOmOmi4vUCQXcDPdWTh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"B_iAjwPm_mYg"},"source":["# Letter recognition (small size)\n","\n","> Indeed, I once even proposed that the toughest challenge facing AI workers is to answer the question: “What are the letters ‘A’ and ‘I’? - [Douglas R. Hofstadter](https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html) (1995)\n","\n","\n","## notMNIST\n","\n","\n","Data source: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) (you need to download `notMNIST_small.mat` file):\n","\n","![](http://yaroslavvb.com/upload/notMNIST/nmn.png)\n","\n","> some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n","\n","> Approaching 0.5% error rate on notMNIST_small would be very impressive. If you run your algorithm on this dataset, please let me know your results.\n","\n","\n","## So, why not MNIST?\n","\n","Many introductions to image classification with deep learning start with MNIST, a standard dataset of handwritten digits. This is unfortunate. Not only does it not produce a “Wow!” effect or show where deep learning shines, but it also can be solved with shallow machine learning techniques. In this case, plain k-Nearest Neighbors produces more than 97% accuracy (or even 99.5% with some data preprocessing!). Moreover, MNIST is not a typical image dataset – and mastering it is unlikely to teach you transferable skills that would be useful for other classification problems\n","\n","> Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and no[t] transfer to real [computer vision]. - [François Chollet’s tweet](https://twitter.com/fchollet/status/852594987527045120)"]},{"cell_type":"code","metadata":{"id":"jcAAphar__K6"},"source":["!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bJ8z5dnANsf"},"source":["import matplotlib.pyplot as plt\n","from scipy import io\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rsvxhuP0_mYt"},"source":["## Data Loading"]},{"cell_type":"code","source":["data = io.loadmat('notMNIST_small.mat')\n","\n","data"],"metadata":{"id":"EGjiGlIJmt5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data['images']\n","y = data['labels']"],"metadata":{"id":"TeNorDxNm0kJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"ync3Gzrim3VI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resolution = 28\n","classes = 10\n","\n","x = np.transpose(x, (2, 0, 1))\n","print(x.shape)\n","x = x.reshape( (-1, resolution, resolution, 1) )"],"metadata":{"id":"NYB_hI-7m58Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample, x, y, channel\n","x.shape, y.shape"],"metadata":{"id":"FPlIBhtrnUbZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 데이터 살펴보기"],"metadata":{"id":"DGxezpUTnv4G"}},{"cell_type":"code","source":["rand_i = np.random.randint(0, x.shape[0])\n","\n","plt.title( f'idx: {rand_i} , y: {\"ABCDEFGHIJ\"[ int(y[rand_i]) ]}' )\n","plt.imshow( x[rand_i, :, :, 0], cmap='Greys' )\n","plt.show()"],"metadata":{"id":"QrHUkjstndza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows = 5\n","fig, axes = plt.subplots(rows, classes, figsize=(classes,rows))\n","\n","for letter_id in range(classes) :\n","    letters = x[y==letter_id]      # 0부터 9까지 각 숫자에 맞는 array가 letters에 들어간다.\n","    letters_len = len(letters)\n","\n","    for row_i in range(rows) :\n","        axe = axes[row_i, letter_id]\n","        axe.imshow( letters[np.random.randint(letters_len)], cmap='Greys', interpolation='none')\n","        axe.axis('off')"],"metadata":{"id":"7vYrb85CsSKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"KG6mnMZkw3iz"}},{"cell_type":"markdown","source":["* Data split\n","\n","    - training set : test set = 8 : 2\n","    - 재연을 위한 난수 고정 : 2023"],"metadata":{"id":"leN2EkZUv8um"}},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"pw1cpJPLw3D9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"KwFP1-V8HcBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=2023)"],"metadata":{"id":"oQQtaOZQv7eQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x.shape, train_y.shape, test_x.shape, test_y.shape"],"metadata":{"id":"HiyE8sW6wQg4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Scaling\n","\n","    - min-max scaling"],"metadata":{"id":"L6wdqmy9xglU"}},{"cell_type":"code","source":["max_n, min_n = train_x.max(), train_x.min()"],"metadata":{"id":"L_H5ZJQ2xgiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x = (train_x - min_n) / (max_n - min_n)\n","test_x = (test_x - min_n) / (max_n - min_n)"],"metadata":{"id":"r0HOD2R_xz20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x.max(), train_x.min()"],"metadata":{"id":"9ciZP9h4x8F1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* One-hot encoding"],"metadata":{"id":"u7VEtyVIxgeW"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"CxL4r3qhxDdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_len = len(np.unique(train_y))\n","class_len"],"metadata":{"id":"UQ7VK412yWPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_y = to_categorical(train_y, class_len)\n","test_y = to_categorical(test_y, class_len)"],"metadata":{"id":"RIRcIJxRyDHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Data shape 재확인"],"metadata":{"id":"X072_BHJzpcF"}},{"cell_type":"code","source":["train_x.shape, train_y.shape"],"metadata":{"id":"luXfEhMHyc1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Modeling\n","- 조건\n","    1. Flatten Layer 사용할 것\n","    2. Activation Function이 주어진 Dense Layer 뒤에 BatchNormalization 사용할 것\n","    3. Dropout을 0.2 정도로 사용할 것\n","    4. Early Stopping을 사용할 것"],"metadata":{"id":"zvuRbktZztOz"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout\n","from tensorflow.keras.layers import Activation"],"metadata":{"id":"dYHoJ85OzspV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Sequential API"],"metadata":{"id":"haBYWbZP0r4i"}},{"cell_type":"code","source":["## BatchNormalization의 최초 저자는 Activation 이전에 적용할 것을 주장!\n","\n","# 1. 세션 클리어\n","clear_session()\n","\n","# 2. 모델 선언\n","model1 = Sequential()\n","\n","# 3. 레이어 블록 조립\n","model1.add( Input(shape=(28,28,1)) )\n","model1.add( Flatten() )\n","model1.add( Dense(256) )\n","model1.add( BatchNormalization() )\n","model1.add( Activation('relu') )\n","model1.add( Dropout(0.2) )\n","model1.add( Dense(128) )\n","model1.add( BatchNormalization() )\n","model1.add( Activation('relu') )\n","model1.add( Dropout(0.2) )\n","model1.add( Dense(64) )\n","model1.add( BatchNormalization() )\n","model1.add( Activation('relu') )\n","model1.add( Dropout(0.2) )\n","model1.add( Dense(10) )\n","model1.add( Activation('softmax') )\n","\n","# 4. 컴파일\n","model1.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model1.summary()"],"metadata":{"id":"1DsOHDw40oBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model"],"metadata":{"id":"rmTta8KU1YzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_model(model1, show_shapes=True, show_layer_activations=True)"],"metadata":{"id":"YcuhRjLu1dRY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Functional API"],"metadata":{"id":"On0Jhp2016YJ"}},{"cell_type":"code","source":["# 그러나 성능은 Activation을 거친 이후에 BatchNormalization을 사용하는게 더 좋다고 알려짐!\n","\n","# 1. 세션 클리어\n","clear_session()\n","\n","# 2. 레이어 엮기\n","il = Input(shape=(28,28,1))\n","hl = Flatten()(il)\n","hl = Dense(256, activation='relu')(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.2)(hl)\n","hl = Dense(128, activation='relu')(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.2)(hl)\n","hl = Dense(64, activation='relu')(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.2)(hl)\n","ol = Dense(10, activation='softmax')(hl)\n","\n","# 3. 모델의 시작과 끝 지정\n","model2 = Model(il, ol)\n","\n","# 4. 컴파일\n","model2.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model2.summary()"],"metadata":{"id":"tE75ckLz15_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Early stopping"],"metadata":{"id":"JLoTURuG3Ud1"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"4yFLLP9f3Xzl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = EarlyStopping(monitor='val_loss',          # 얼리 스토핑을 적용할 관측 대상\n","                   min_delta=0,                 # Threshold. 설정한 값 이상으로 변해야 성능 개선되었다고 간주.\n","                   patience=3,                  # 성능 개선이 발생하지 않았을 때, 몇 epoch를 더 지켜볼 것인가.\n","                   verbose=1,\n","                   restore_best_weights=True)   # 성능이 가장 좋은 epoch의 가중치를 적용함."],"metadata":{"id":"Fc9ravjK3lzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .fit( )"],"metadata":{"id":"zVbVljD627ws"}},{"cell_type":"code","source":["model1.fit(train_x, train_y, epochs=20, verbose=1,\n","           validation_split=0.2,  # 매 epoch마다 training set의 20%를 validation set으로 만듬\n","           callbacks=[es]         # 얼리스토핑 적용\n","           )"],"metadata":{"id":"-qu1a2xC21-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .evaluate( )"],"metadata":{"id":"JC-yQqn33KqM"}},{"cell_type":"code","source":["model1.evaluate(test_x, test_y, verbose=1)"],"metadata":{"id":"UMD-WEqc3A60"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .predict( )"],"metadata":{"id":"R4iXxcP53BXn"}},{"cell_type":"code","source":["y_pred = model1.predict(test_x)"],"metadata":{"id":"gEIsa3Rr3BBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원핫 인코딩 한 것을 다시 묶어주는 코드\n","# 평가 지표 및 실제 데이터 확인을 위해 필요\n","\n","y_pred_arg = np.argmax(y_pred, axis=1)\n","test_y_arg = np.argmax(test_y, axis=1)"],"metadata":{"id":"ERbul_XPMPAY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 평가 지표"],"metadata":{"id":"h0i7gtoKQI2M"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"V0bKlDKAJO6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(test_y_arg, y_pred_arg)"],"metadata":{"id":"xKaSctmxJSR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print( classification_report(test_y_arg, y_pred_arg) )"],"metadata":{"id":"cFCAI30pInVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 실제 데이터 확인"],"metadata":{"id":"iAz06dlD7Gno"}},{"cell_type":"code","source":["letters_str = \"ABCDEFGHIJ\"\n","\n","rand_idx = np.random.randint(0, len(y_pred_arg))\n","test_idx = test_y_arg[rand_idx]\n","pred_idx = y_pred_arg[rand_idx]\n","class_prob = np.floor( y_pred[rand_idx]*100 )\n","\n","print(f'idx = {rand_idx}')\n","print(f'해당 인덱스의 이미지는 {letters_str[test_idx]}')\n","print(f'모델의 예측 : {letters_str[pred_idx]}')\n","print(f'모델의 클래스별 확률 : ')\n","print('-------------------')\n","for idx, val in enumerate(letters_str) :\n","    print(val, class_prob[idx])\n","print('=================================================')\n","\n","if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :\n","    print('정답')\n","else :\n","    print('땡')\n","\n","plt.imshow(test_x[rand_idx], cmap='Greys')\n","plt.show()"],"metadata":{"id":"rDzw-qDE7Eoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 틀린 이미지만 확인해보기"],"metadata":{"id":"v7lHIf-B9Z_L"}},{"cell_type":"code","source":["temp = (test_y_arg == y_pred_arg)\n","false_idx = np.where(temp==False)[0]\n","false_len = len(false_idx)\n","false_len"],"metadata":{"id":"WnLoTiF0LZEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["letters_str = \"ABCDEFGHIJ\"\n","\n","rand_idx = false_idx[np.random.randint(0, false_len)]\n","test_idx = test_y_arg[rand_idx]\n","pred_idx = y_pred_arg[rand_idx]\n","class_prob = np.floor( y_pred[rand_idx]*100 )\n","\n","print(f'idx = {rand_idx}')\n","print(f'해당 인덱스의 이미지는 {letters_str[test_idx]}')\n","print(f'모델의 예측 : {letters_str[pred_idx]}')\n","print(f'모델의 클래스별 확률 : ')\n","print('-------------------')\n","for idx, val in enumerate(letters_str) :\n","    print(val, class_prob[idx])\n","print('=================================================')\n","\n","if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :\n","    print('정답')\n","else :\n","    print('땡')\n","\n","plt.imshow(test_x[rand_idx], cmap='Greys')\n","plt.show()"],"metadata":{"id":"bWcCLIW2_ACh"},"execution_count":null,"outputs":[]}]}