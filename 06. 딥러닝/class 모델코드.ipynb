{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class 모델코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 코드 생성 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 1번 - 기몬 모델을 클래스 모델로\n",
        "\n",
        "# X = tf.keras.Input(shape=[1])\n",
        "# Y = tf.keras.layers.Dense(1)(X)\n",
        "# model = tf.keras.Model(X, Y)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(loss='mse')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 2번 - hidden layers 모델을 클래스 모델로\n",
        "\n",
        "# X = tf.keras.Input(shape=[1])\n",
        "# H = tf.keras.layers.Dense(2, activation='swish')(X)\n",
        "# Y = tf.keras.layers.Dense(1)(H)\n",
        "# model = tf.keras.Model(X, Y)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.dense1 = tf.keras.layers.Dense(2, activation='swish')\n",
        "        self.dense2 = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, X):\n",
        "        H = self.dense1(X)\n",
        "        Y = self.dense2(H)\n",
        "        return Y\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 방법 3번 - dropout 추가\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
        "    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    x = self.dense1(inputs)\n",
        "    if training:\n",
        "      x = self.dropout(x, training=training)\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/blackdew/ml-tensorflow/master/data/csv/lemonade.csv\")\n",
        "x_train = df[['온도']]\n",
        "y_train = df[['판매량']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model.predict(x_train))\n",
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. fasion model class 생성 실습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 원하는 모델!!\n",
        "# X = tf.keras.Input(shape=[28, 28])\n",
        "# H = tf.keras.layers.Flatter()(X)\n",
        "# H = tf.keras.layers.Dense(128, activation='swish')(H)\n",
        "# H = tf.keras.layers.BatchNormalization()(H)\n",
        "# H = tf.keras.layers.Dropout(0.3)(H)\n",
        "# H = tf.keras.layers.Dense(32, activation='swish')(X)\n",
        "# H = tf.keras.layers.BatchNormalization()(H)\n",
        "# H = tf.keras.layers.Dropout(0.3)(H)\n",
        "# Y = tf.keras.layers.Dense(10, activation='softmax')(H)\n",
        "# model = tf.keras.Model(X, Y)\n",
        "# model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
        "# model.summary()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MyModel, self).__init__(**kwargs)\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    \n",
        "    self.dense1 = tf.keras.layers.Dense(64, activation=tf.nn.swish)\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "    \n",
        "    self.dense2 = tf.keras.layers.Dense(20, activation=tf.nn.swish)\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "    \n",
        "    self.dense3 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "    \n",
        "\n",
        "  def call(self, X):\n",
        "    H = self.flatten(X)\n",
        "    H = self.dense1(H)\n",
        "    H = self.bn1(H)\n",
        "    H = self.dense2(H)\n",
        "    H = self.bn2(H)\n",
        "    Y = self.dense3(H)\n",
        "    return Y\n",
        "\n",
        "fasion_model = MyModel()\n",
        "fasion_model.compile(optimizer='adam', loss='spaese_categorical_crossentropy', metrics='accuracy')\n",
        "fasion_model.build(input_shape=[28, 28])\n",
        "fasion_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_fashion_mnist_model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_14 (Flatten)        multiple                  0         \n",
            "                                                                 \n",
            " dense_71 (Dense)            multiple                  50240     \n",
            "                                                                 \n",
            " batch_normalization_28 (Ba  multiple                  256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " dense_72 (Dense)            multiple                  1300      \n",
            "                                                                 \n",
            " batch_normalization_29 (Ba  multiple                  80        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        multiple                  0         \n",
            "                                                                 \n",
            " dense_73 (Dense)            multiple                  210       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52086 (203.46 KB)\n",
            "Trainable params: 51918 (202.80 KB)\n",
            "Non-trainable params: 168 (672.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델을 준비합니다.\n",
        "# X = tf.keras.Input(shape=[28, 28])\n",
        "# H = tf.keras.layers.Flatten()(X)\n",
        "# H = tf.keras.layers.Dense(64, activation=tf.keras.activations.swish)(H)\n",
        "# H = tf.keras.layers.BatchNormalization(H)\n",
        "# H = tf.keras.layers.Dense(20, activation=tf.keras.activations.swish)(H)\n",
        "# H = tf.keras.layers.BatchNormalization(H)\n",
        "# Y = tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)(H)\n",
        "# model = tf.keras.Model(X, Y)\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "#               # loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "#               # metrics=tf.keras.metrics.sparse_categorical_accuracy,\n",
        "#               metrics=tf.keras.metrics.SparseCategoricalAccuracy())\n",
        "# model.summary()\n",
        "\n",
        "class MyFashionMNISTModel(tf.keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyFashionMNISTModel, self).__init__(**kwargs)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation=\"swish\")\n",
        "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
        "        self.dropout1 = tf.keras.layers.Dropout(0.5)\n",
        "        self.dense2 = tf.keras.layers.Dense(20, activation=\"swish\")\n",
        "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
        "        self.dropout2 = tf.keras.layers.Dropout(0.5)\n",
        "        self.dense3 = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "\n",
        "    def call(self, X):\n",
        "        H = self.flatten(X)\n",
        "        H = self.dense1(H)\n",
        "        H = self.bn1(H)\n",
        "        H = self.dropout1(H)\n",
        "        H = self.dense2(H)\n",
        "        H = self.bn2(H)\n",
        "        H = self.dropout2(H)\n",
        "        Y = self.dense3(H)\n",
        "        return Y\n",
        "    \n",
        "    def train_step(self, batch):\n",
        "        x_batch, y_batch = batch\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x_batch, training=True)\n",
        "            loss = self.compiled_loss(y_batch, y_pred)\n",
        "\n",
        "        grad = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grad, self.trainable_weights))\n",
        "\n",
        "        self.compiled_metrics.update_state(y_batch, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, batch):\n",
        "        x_batch, y_batch = batch\n",
        "        y_pred = self(x_batch)\n",
        "        loss = self.compiled_loss(y_batch, y_pred)\n",
        "\n",
        "        self.compiled_metrics.update_state(y_batch, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "model = MyFashionMNISTModel()\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
        "model.build(input_shape=[None, 28, 28])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train, x_val = x_train[:48000], x_train[48000:]\n",
        "y_train, y_val = y_train[:48000], y_train[48000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 2s 3ms/step - loss: 1.1995 - accuracy: 0.5894 - val_loss: 0.5680 - val_accuracy: 0.8099\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.7875 - accuracy: 0.7295 - val_loss: 0.5031 - val_accuracy: 0.8198\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.7630 - val_loss: 0.4516 - val_accuracy: 0.8374\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.7818 - val_loss: 0.4348 - val_accuracy: 0.8449\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.7904 - val_loss: 0.4484 - val_accuracy: 0.8325\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5974 - accuracy: 0.8002 - val_loss: 0.4218 - val_accuracy: 0.8480\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5855 - accuracy: 0.8015 - val_loss: 0.4068 - val_accuracy: 0.8525\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5750 - accuracy: 0.8053 - val_loss: 0.4147 - val_accuracy: 0.8520\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5640 - accuracy: 0.8122 - val_loss: 0.4035 - val_accuracy: 0.8478\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5588 - accuracy: 0.8133 - val_loss: 0.4035 - val_accuracy: 0.8572\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5613 - accuracy: 0.8118 - val_loss: 0.4035 - val_accuracy: 0.8530\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5450 - accuracy: 0.8172 - val_loss: 0.3939 - val_accuracy: 0.8568\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.8177 - val_loss: 0.4139 - val_accuracy: 0.8426\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5358 - accuracy: 0.8192 - val_loss: 0.3785 - val_accuracy: 0.8626\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5332 - accuracy: 0.8217 - val_loss: 0.3876 - val_accuracy: 0.8586\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5308 - accuracy: 0.8231 - val_loss: 0.3840 - val_accuracy: 0.8608\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.8239 - val_loss: 0.3864 - val_accuracy: 0.8613\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.8232 - val_loss: 0.3988 - val_accuracy: 0.8594\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.8241 - val_loss: 0.3869 - val_accuracy: 0.8600\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.8246 - val_loss: 0.3999 - val_accuracy: 0.8539\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.8248 - val_loss: 0.3843 - val_accuracy: 0.8641\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5134 - accuracy: 0.8276 - val_loss: 0.3794 - val_accuracy: 0.8657\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5067 - accuracy: 0.8305 - val_loss: 0.3889 - val_accuracy: 0.8624\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.5056 - accuracy: 0.8292 - val_loss: 0.3861 - val_accuracy: 0.8640\n"
          ]
        }
      ],
      "source": [
        "# model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_var, y_var))\n",
        "\n",
        "\n",
        "early = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # 오버피팅이 10번 관측되면 멈추고, 여태 나온 최고 성능으로 모델을 만들어라\n",
        "result = model.fit(x_train, y_train, epochs=100, batch_size=128,\n",
        "                   validation_data=(x_val, y_val),\n",
        "                   callbacks=[early]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## packing / unpacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# list의 경우\n",
        "# packing\n",
        "a, *b = [1,2,3,4]\n",
        "print(a, b)\n",
        "\n",
        "# unpacking\n",
        "c = [1, b]\n",
        "d = [1, *b]\n",
        "print(c)\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dictionary의 경우\n",
        "\n",
        "# dicionary unpacking\n",
        "a = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
        "b = {**a, \"d\": 4, \"e\": 5} \n",
        "print(b)\n",
        "\n",
        "# dicionary packing 불가능\n",
        "a, **b = {\"a\": 1, \"b\": 2, \"c\": 3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 함수에서 사용하는 경우\n",
        "\n",
        "# 함수에서 list packing\n",
        "def add(a, b, c):\n",
        "    return a + b + c\n",
        "\n",
        "d = [1,2,3]\n",
        "print(add(d[0],d[1],d[2])) # 우리가 packing을 몰랐다면....\n",
        "print(add(*d)) # packing을 아니까 이렇게 편하다\n",
        "\n",
        "print(\"-\"*50)\n",
        "\n",
        "# 함수에서 dictionary packing\n",
        "def add(**a):\n",
        "    print(a)\n",
        "    return sum(a.values())\n",
        "\n",
        "print(add(a=1))\n",
        "print(add(a=1, b=2, c=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# packing\n",
        "def add(*a):\n",
        "    print(a)\n",
        "    return sum(a)\n",
        "\n",
        "print(add(1))\n",
        "print(add(1, 2))\n",
        "print(add(1, 2, 3))\n",
        "print(add(1, 2, 3, 4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
